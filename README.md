<img width="130" height="130" alt="LLM_booster" src="https://github.com/user-attachments/assets/66e10a67-78a5-4854-87d9-e1acc88e8636" />


# LLM Test-Time Scaling Service

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A research framework for implementing and evaluating test-time scaling strategies for large language models. Includes implementations of DeepConf, Best-of-N, Self-Consistency, and Chain-of-Thought strategies.

---

## üöÄ Quick Start

### For Users (Run Experiments)

```bash
# 1. Install
./setup.sh

# 2. Configure API keys
cp .env.example .env
# Edit .env and add your OPENROUTER_API_KEY

# 3. Run DeepConf on GSM8K
python scripts/run_tts_eval.py \
  --config-name experiments/deepconf/run_gsm8k_deepconf_offline \
  dataset.subset=10
```

### For Developers (Contribute Code)

See [Onboarding Guide](#-onboarding-for-developers) below.

---

## üìö Onboarding for Developers

**Welcome!** Follow these steps to get started with development:

### Step 1: Understand the Project

**Read the documentation:**
- **[Project Structure](docs/PROJECT_STRUCTURE.md)** - Architecture overview, components, design patterns
- **[Strategy Registration](docs/STRATEGY_REGISTRATION.md)** - How to add new strategies with tests
- **[DeepConf Guide](docs/deepconf/DeepConf.md)** - Example strategy implementation

**Quick architecture overview:**
```
llm_tts/strategies/     ‚Üí TTS strategy implementations
llm_tts/models/         ‚Üí Model wrappers with streaming support
llm_tts/scorers/        ‚Üí Step scoring functions (PRM, uncertainty)
llm_tts/evaluation/     ‚Üí Correctness evaluation methods
config/                 ‚Üí Hydra configuration system
tests/                  ‚Üí Test suite with strategy registry
```

### Step 2: Set Up Development Environment

```bash
# Clone the repository
git clone https://github.com/IINemo/llm-tts-service.git
cd llm-tts-service

# Install dependencies and lm-polygraph
./setup.sh

# Install dev dependencies and git hooks
pip install -e ".[dev]"
make hooks
```

**What this does:**
- Installs package in editable mode (`-e`)
- Installs lm-polygraph dev branch (submodule)
- Sets up pre-commit hooks (black, isort, flake8)

### Step 3: Configure API Keys

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your keys:
# OPENROUTER_API_KEY=sk-or-v1-...
# DEEPSEEK_API_KEY=sk-...
```

**Required for:**
- Running experiments (OPENROUTER_API_KEY)
- Evaluation with LLM judge (DEEPSEEK_API_KEY or OPENROUTER_API_KEY)

### Step 4: Verify Installation

```bash
# Run tests to verify setup
pytest tests/strategy_registry.py --validate  # Validate registry
pytest tests/deepconf/ -v                     # Run DeepConf tests
pytest tests/online_best_of_n/ -v             # Run Best-of-N tests

# Or run all tests
make test
```

**Expected result:** All tests pass (some may skip if API keys not set)

### Step 5: Run Your First Experiment

```bash
# Quick test with 1 sample
python scripts/run_tts_eval.py \
  --config-name experiments/deepconf/run_gsm8k_deepconf_offline \
  dataset.subset=1 \
  strategy.budget=4

# Check the output
ls outputs/  # Results saved here with timestamp
```

### Step 6: Make Your First Change

**Example: Add a new strategy**

```bash
# 1. Create strategy file
touch llm_tts/strategies/strategy_my_new.py

# 2. Implement your strategy (inherit from StrategyBase)

# 3. Create tests
mkdir tests/my_new
touch tests/my_new/test_my_new.py

# 4. Register in strategy registry
# Edit tests/strategy_registry.py

# 5. Validate
python tests/strategy_registry.py --validate

# 6. Run tests
pytest tests/my_new/ -v
```

**See [Strategy Registration Guide](docs/STRATEGY_REGISTRATION.md) for detailed steps.**

### Step 7: Daily Development Workflow

```bash
# Make your changes...

# Format and check before committing
make fix     # Auto-fix with black, isort
make lint    # Check with flake8

# Run relevant tests
pytest tests/your_module/ -v

# Commit (hooks run automatically)
git commit -m "feat: add new feature"

# Push
git push origin your-branch
```

**Pre-commit hooks will:**
- Format code with black and isort
- Check for trailing whitespace, large files
- Run flake8 linting
- Block commit if checks fail

---

## üìÅ Project Structure

```
llm-tts-service/
‚îú‚îÄ‚îÄ config/              # Hydra configuration (experiments, models, strategies)
‚îú‚îÄ‚îÄ llm_tts/             # Main library
‚îÇ   ‚îú‚îÄ‚îÄ strategies/      # TTS strategy implementations (DeepConf, Best-of-N, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Model wrappers (streaming, early stopping)
‚îÇ   ‚îú‚îÄ‚îÄ scorers/         # Step scoring (PRM, uncertainty, voting)
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/      # Correctness evaluation (LLM judge, exact match)
‚îÇ   ‚îî‚îÄ‚îÄ datasets/        # Dataset utilities (GSM8K, etc.)
‚îú‚îÄ‚îÄ scripts/             # Main evaluation script (run_tts_eval.py)
‚îú‚îÄ‚îÄ tests/               # Test suite with strategy registry
‚îú‚îÄ‚îÄ docs/                # Documentation
‚îî‚îÄ‚îÄ lm-polygraph/        # Submodule: uncertainty estimation
```

**Quick Overview:**
- **Strategies**: DeepConf (confidence-based), Best-of-N (PRM scoring), Self-Consistency, Chain-of-Thought
- **Configuration**: Hierarchical Hydra configs - see `config/README.md`
- **Evaluation**: Two-phase pipeline (generation ‚Üí evaluation) with multi-evaluator support
- **Testing**: Strategy registry enforces test coverage - see [Strategy Registration Guide](docs/STRATEGY_REGISTRATION.md)

**üìñ For detailed architecture and component descriptions, see [Project Structure Documentation](docs/PROJECT_STRUCTURE.md)**

---

## üîß Development Commands

```bash
# Testing
make test              # Run all tests
pytest tests/path/ -v  # Run specific tests

# Code Quality
make fix               # Auto-fix formatting (black, isort)
make format            # Format only (no other hooks)
make lint              # Check with flake8
make hooks             # Install pre-commit hooks

# Validation
python tests/strategy_registry.py --validate  # Validate all strategies
python tests/strategy_registry.py --list      # List registered strategies
```

## üí° Usage Examples

**Note:** API keys are loaded from `.env` file - no need to specify them in the command.

### DeepConf Strategy

**Offline mode** (generate N traces, filter by confidence, majority vote):
```bash
python scripts/run_tts_eval.py \
  --config-name experiments/deepconf/run_gsm8k_deepconf_offline \
  model.model_path="openai/gpt-3.5-turbo" \
  dataset.subset=10
```

**Online mode** (adaptive generation with confidence-based early stopping):
```bash
python scripts/run_tts_eval.py \
  --config-name experiments/deepconf/run_gsm8k_deepconf_online \
  model.model_path="openai/gpt-3.5-turbo" \
  dataset.subset=10
```

### Best-of-N Strategy

With Qwen-3 (local model):
```bash
WANDB_ENTITY=nlpresearch.group WANDB_PROJECT=tts python scripts/run_tts_eval.py \
  --config-path ../config \
  --config-name run_tts_eval \
  dataset=small_gsm8k \
  dataset.subset=1 \
  model=hf_qwen3
```

With ChatGPT via OpenRouter:
```bash
WANDB_ENTITY=nlpresearch.group WANDB_PROJECT=tts python scripts/run_tts_eval.py \
  --config-path ../config \
  --config-name run_tts_eval \
  dataset=small_gsm8k \
  dataset.subset=1 \
  model=openai \
  model.model_path="gpt-4o-mini"
```

With uncertainty scorer:
```bash
WANDB_ENTITY=nlpresearch.group WANDB_PROJECT=tts python scripts/run_tts_eval.py \
  --config-path ../config \
  --config-name run_tts_eval \
  dataset=small_gsm8k \
  dataset.subset=1 \
  model=hf_qwen3 \
  scorer=uncertainty
```

---

## üõ°Ô∏è Robustness & Resume Features

**Evaluations are crash-resistant** - results saved after each sample, resume from interruptions with `--resume`.

### Key Features

- ‚úÖ **Incremental Saving**: No work lost - saves after each sample (not batched)
- ‚úÖ **Resume Capability**: Continue from where you left off with `--resume` or `--resume-from`
- ‚úÖ **Full Reproducibility**: Every run saves complete config snapshot in `.hydra/`

### Quick Resume

```bash
# Resume from latest
python scripts/run_tts_eval.py --config-name your_experiment --resume

# Resume from specific run
python scripts/run_tts_eval.py --resume-from outputs/2025-10-18/23-50-46
```

**üìñ For detailed documentation, troubleshooting, and best practices, see [Robustness Guide](docs/ROBUSTNESS.md)**

---

## üåê REST API Service (Optional)

Deploy strategies as a REST API for production use.

### Quick Start

```bash
./start_service_app.sh  # Automated setup with Docker
```

### Manual Setup

```bash
# With Docker
export OPENROUTER_API_KEY="your-key"
docker-compose up -d

# Without Docker (local dev)
pip install -e ".[service]"
export OPENROUTER_API_KEY="your-key"
python service_app/main.py
```

**See `service_app/README.md` for API documentation.**

---

## üìñ Documentation

- **[Project Structure](docs/PROJECT_STRUCTURE.md)** - Detailed architecture and components
- **[Strategy Registration](docs/STRATEGY_REGISTRATION.md)** - Adding new strategies with tests
- **[Robustness Guide](docs/ROBUSTNESS.md)** - Incremental saving, resume, and reproducibility
- **[DeepConf Guide](docs/deepconf/DeepConf.md)** - Confidence-based test-time scaling
- **[GSM8K Dataset](docs/datasets/GSM8K/)** - Dataset usage examples
- **[Configuration Guide](config/README.md)** - Hydra config system

---

## ü§ù Contributing

1. Read the [Onboarding Guide](#-onboarding-for-developers)
2. Check [Strategy Registration](docs/STRATEGY_REGISTRATION.md) for requirements
3. Follow the [Daily Workflow](#step-7-daily-development-workflow)
4. Ensure tests pass: `make test`
5. Submit a PR

---

## üìù TODO

- Add new scorers (semantic similarity, calibration-based)
- Implement Tree of Thought strategy
- Add MATH dataset support

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
