{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Results Analysis\n",
    "\n",
    "Pull experiment metrics from Weights & Biases, build comparison tables, and export LaTeX for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import wandb\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\nexcept ImportError:\n    sns = None\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", 40)\npd.set_option(\"display.width\", 200)\n\napi = wandb.Api()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Configuration ──────────────────────────────────────────────────────────────\nWANDB_ENTITY = \"nlpresearch.group\"\n\n# Map a short label to the wandb project name\nPROJECTS = {\n    \"aime2024\": \"llm-tts-eval-aime24\",\n    \"aime2025\": \"llm-tts-eval-aime25\",\n    \"minerva_math\": \"llm-tts-eval-minerva-math\",\n    \"math500\": \"llm-tts-eval-math500\",\n    \"gaokao2023en\": \"llm-tts-eval-gaokao2023en\",\n    \"olympiadbench\": \"llm-tts-eval-olympiadbench\",\n    \"gpqa_diamond\": \"llm-tts-eval-gpqa-diamond\",\n}\n\n# Which evaluator metric to use as the primary accuracy column\nPRIMARY_EVALUATOR = \"exact_match\"  # or \"llm_judge\"\n\n# Optional: only keep runs whose group matches one of these substrings.\n# Set to None to keep everything.\nGROUP_FILTERS = None  # e.g. [\"beam_search\", \"offline_bon\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data Fetching ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def fetch_runs(project: str, filters: dict | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Pull all runs from a wandb project and return a flat DataFrame.\"\"\"\n",
    "    path = f\"{WANDB_ENTITY}/{project}\"\n",
    "    runs = api.runs(path, filters=filters or {})\n",
    "\n",
    "    records = []\n",
    "    for run in runs:\n",
    "        cfg = run.config\n",
    "        s = run.summary._json_dict\n",
    "\n",
    "        # Nested config access helpers\n",
    "        strategy_cfg = cfg.get(\"strategy\", {})\n",
    "        scorer_cfg = cfg.get(\"scorer\", {})\n",
    "        model_cfg = cfg.get(\"model\", {})\n",
    "        dataset_cfg = cfg.get(\"dataset\", {})\n",
    "        system_cfg = cfg.get(\"system\", {})\n",
    "\n",
    "        record = {\n",
    "            # identifiers\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            \"group\": run.group,\n",
    "            \"state\": run.state,\n",
    "            \"project\": project,\n",
    "            # config fields\n",
    "            \"strategy\": strategy_cfg.get(\"type\"),\n",
    "            \"scorer\": scorer_cfg.get(\"type\"),\n",
    "            \"aggregation\": strategy_cfg.get(\"aggregation\"),\n",
    "            \"scoring_window\": strategy_cfg.get(\"scoring_window\"),\n",
    "            \"scoring_window_label\": strategy_cfg.get(\"scoring_window_label\"),\n",
    "            \"model\": model_cfg.get(\"model_short_name\") or model_cfg.get(\"model_name\"),\n",
    "            \"dataset\": dataset_cfg.get(\"data_name\"),\n",
    "            \"seed\": system_cfg.get(\"seed\"),\n",
    "            \"beam_size\": strategy_cfg.get(\"beam_size\"),\n",
    "            \"candidates_per_beam\": strategy_cfg.get(\"candidates_per_beam\"),\n",
    "            \"num_paths\": strategy_cfg.get(\"num_paths\"),\n",
    "            \"num_candidates\": strategy_cfg.get(\"num_candidates\"),\n",
    "            \"max_steps\": strategy_cfg.get(\"max_steps\"),\n",
    "            # summary metrics\n",
    "            \"exact_match\": s.get(\"exact_match/accuracy\"),\n",
    "            \"llm_judge_accuracy\": s.get(\"llm_judge/accuracy\"),\n",
    "            \"avg_reasoning_steps\": s.get(\"avg_reasoning_steps_per_trajectory\"),\n",
    "            \"total_tokens\": s.get(\"compute/total_tokens\"),\n",
    "            \"total_input_tokens\": s.get(\"compute/total_input_tokens\"),\n",
    "            \"total_output_tokens\": s.get(\"compute/total_output_tokens\"),\n",
    "            \"total_tflops\": s.get(\"compute/total_tflops\"),\n",
    "            \"avg_tokens_per_sample\": s.get(\"compute/avg_tokens_per_sample\"),\n",
    "            \"avg_output_tokens_per_sample\": s.get(\"compute/avg_output_tokens_per_sample\"),\n",
    "            \"avg_tflops_per_sample\": s.get(\"compute/avg_tflops_per_sample\"),\n",
    "            \"total_generations\": s.get(\"compute/total_generations\"),\n",
    "            \"prm_tflops\": s.get(\"compute/prm_tflops\"),\n",
    "            \"total_samples\": s.get(\"total_samples\"),\n",
    "            \"completed\": s.get(\"completed\"),\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Fetch all projects\n",
    "dfs = []\n",
    "for label, proj in PROJECTS.items():\n",
    "    print(f\"Fetching {label} ({proj})...\")\n",
    "    try:\n",
    "        df = fetch_runs(proj)\n",
    "        df[\"project_label\"] = label\n",
    "        dfs.append(df)\n",
    "        print(f\"  -> {len(df)} runs\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> ERROR: {e}\")\n",
    "\n",
    "raw_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "print(f\"\\nTotal raw runs: {len(raw_df)}\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Data Cleaning & Parsing ───────────────────────────────────────────────────\n\ndef parse_group_name(group: str | None) -> dict:\n    \"\"\"Best-effort extraction of structured fields from the group name.\n\n    Expected patterns:\n      {strategy}_{model}_{dataset}\n      {strategy}_{model}_{dataset}_{scorer}\n      {strategy}_{model}_{dataset}_{scorer}_{window}_{aggregation}\n    \"\"\"\n    result = {\"_group_strategy\": None, \"_group_model\": None,\n              \"_group_dataset\": None, \"_group_scorer\": None,\n              \"_group_window\": None, \"_group_aggregation\": None}\n    if not group:\n        return result\n\n    known_strategies = {\n        \"baseline\", \"chain_of_thought\", \"self_consistency\",\n        \"online_bon\", \"offline_bon\", \"beam_search\",\n        \"uncertainty_cot\", \"extended_thinking\",\n        \"adaptive_scaling\", \"deepconf\",\n    }\n    known_scorers = {\n        \"prm\", \"entropy\", \"perplexity\", \"sequence_prob\",\n        \"uncertainty\", \"uncertainty_pd\", \"uncertainty_uhead\",\n    }\n    known_aggregations = {\"mean\", \"min\", \"max\", \"sum\", \"product\", \"median\"}\n    known_datasets = {\n        \"minerva_math\", \"math500\", \"aime2024\", \"aime2025\",\n        \"gaokao2023en\", \"human_eval_plus\", \"olympiadbench\",\n    }\n\n    parts = group.split(\"_\")\n\n    # Greedy match strategy prefix (try longest first)\n    strategy = None\n    for length in range(min(3, len(parts)), 0, -1):\n        candidate = \"_\".join(parts[:length])\n        if candidate in known_strategies:\n            strategy = candidate\n            parts = parts[length:]\n            break\n    result[\"_group_strategy\"] = strategy\n\n    # Scan remaining parts for known tokens\n    remaining = \"_\".join(parts)\n    for ds in sorted(known_datasets, key=len, reverse=True):\n        if ds in remaining:\n            result[\"_group_dataset\"] = ds\n            remaining = remaining.replace(ds, \"\", 1)\n            break\n    for sc in sorted(known_scorers, key=len, reverse=True):\n        if f\"_{sc}\" in f\"_{remaining}\":\n            result[\"_group_scorer\"] = sc\n            remaining = remaining.replace(sc, \"\", 1)\n            break\n    for ag in known_aggregations:\n        if f\"_{ag}\" in f\"_{remaining}\":\n            result[\"_group_aggregation\"] = ag\n            break\n    # window: look for a bare integer or \"all\"\n    for p in remaining.split(\"_\"):\n        if p.isdigit():\n            result[\"_group_window\"] = p\n            break\n        if p == \"all\":\n            result[\"_group_window\"] = \"all\"\n            break\n\n    # model: whatever remains after removing known tokens is likely the model\n    for tok in [result[\"_group_dataset\"], result[\"_group_scorer\"],\n                result[\"_group_aggregation\"], result[\"_group_window\"]]:\n        if tok:\n            remaining = remaining.replace(tok, \"\", 1)\n    model_str = \"_\".join(p for p in remaining.split(\"_\") if p)\n    result[\"_group_model\"] = model_str or None\n\n    return result\n\n\ndf = raw_df.copy()\n\n# Parse group names to fill missing config columns\nparsed = df[\"group\"].apply(parse_group_name).apply(pd.Series)\ndf = pd.concat([df, parsed], axis=1)\n\n# Fill missing config from parsed group name\nfor col, gcol in [(\"strategy\", \"_group_strategy\"), (\"scorer\", \"_group_scorer\"),\n                   (\"aggregation\", \"_group_aggregation\"),\n                   (\"scoring_window\", \"_group_window\"),\n                   (\"dataset\", \"_group_dataset\"), (\"model\", \"_group_model\")]:\n    df[col] = df[col].fillna(df[gcol])\n\n# Drop helper columns\ndf.drop(columns=[c for c in df.columns if c.startswith(\"_group_\")], inplace=True)\n\n# Filter to finished runs only\nn_before = len(df)\ndf = df[df[\"state\"] == \"finished\"].copy()\nprint(f\"Kept {len(df)}/{n_before} finished runs\")\n\n# Optional group filter\nif GROUP_FILTERS:\n    mask = df[\"group\"].apply(lambda g: any(f in (g or \"\") for f in GROUP_FILTERS))\n    df = df[mask].copy()\n    print(f\"After group filter: {len(df)} runs\")\n\n# Normalize accuracy to percentage\nfor col in [\"exact_match\", \"llm_judge_accuracy\"]:\n    if col in df.columns:\n        # If values look like fractions (0-1), convert to pct\n        mask = df[col].notna() & (df[col] <= 1.0)\n        df.loc[mask, col] = df.loc[mask, col] * 100\n\nprint(f\"\\nStrategies: {sorted(df['strategy'].dropna().unique())}\")\nprint(f\"Scorers:    {sorted(df['scorer'].dropna().unique())}\")\nprint(f\"Datasets:   {sorted(df['dataset'].dropna().unique())}\")\ndf[[\"strategy\", \"scorer\", \"aggregation\", \"scoring_window\", \"dataset\", \"model\",\n    \"exact_match\", \"total_tflops\"]].head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Seed Averaging ────────────────────────────────────────────────────────────\n",
    "\n",
    "CONFIG_COLS = [\"strategy\", \"scorer\", \"aggregation\", \"scoring_window\",\n",
    "               \"model\", \"dataset\", \"project_label\",\n",
    "               \"beam_size\", \"candidates_per_beam\", \"num_paths\", \"num_candidates\"]\n",
    "\n",
    "METRIC_COLS = [\"exact_match\", \"llm_judge_accuracy\", \"avg_reasoning_steps\",\n",
    "               \"total_tokens\", \"total_tflops\", \"avg_tokens_per_sample\",\n",
    "               \"avg_output_tokens_per_sample\", \"avg_tflops_per_sample\"]\n",
    "\n",
    "\n",
    "def aggregate_seeds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Group by config columns and compute mean/std over seeds.\"\"\"\n",
    "    present_cfg = [c for c in CONFIG_COLS if c in df.columns]\n",
    "    present_met = [c for c in METRIC_COLS if c in df.columns]\n",
    "\n",
    "    grouped = df.groupby(present_cfg, dropna=False)\n",
    "    agg = grouped[present_met].agg([\"mean\", \"std\", \"count\"]).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    flat_cols = []\n",
    "    for col in agg.columns:\n",
    "        if isinstance(col, tuple) and col[1]:\n",
    "            flat_cols.append(f\"{col[0]}_{col[1]}\")\n",
    "        else:\n",
    "            flat_cols.append(col[0] if isinstance(col, tuple) else col)\n",
    "    agg.columns = flat_cols\n",
    "\n",
    "    # Add a formatted \"mean +/- std\" column for the primary metric\n",
    "    for m in present_met:\n",
    "        mean_col, std_col = f\"{m}_mean\", f\"{m}_std\"\n",
    "        if mean_col in agg.columns:\n",
    "            agg[f\"{m}_fmt\"] = agg.apply(\n",
    "                lambda r: f\"{r[mean_col]:.1f} +/- {r[std_col]:.1f}\"\n",
    "                if pd.notna(r[std_col]) and r.get(f\"{m}_count\", 0) > 1\n",
    "                else (f\"{r[mean_col]:.1f}\" if pd.notna(r[mean_col]) else \"\"),\n",
    "                axis=1,\n",
    "            )\n",
    "    return agg\n",
    "\n",
    "\n",
    "agg_df = aggregate_seeds(df)\n",
    "print(f\"Aggregated configs: {len(agg_df)}\")\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pivot Table Helper ────────────────────────────────────────────────────────\n",
    "\n",
    "def make_comparison_table(\n",
    "    df: pd.DataFrame,\n",
    "    row_field: str,\n",
    "    col_field: str,\n",
    "    value_field: str = \"exact_match_fmt\",\n",
    "    filter_dict: dict | None = None,\n",
    "    title: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Build a pivot table from the aggregated DataFrame.\"\"\"\n",
    "    sub = df.copy()\n",
    "    if filter_dict:\n",
    "        for k, v in filter_dict.items():\n",
    "            if isinstance(v, list):\n",
    "                sub = sub[sub[k].isin(v)]\n",
    "            else:\n",
    "                sub = sub[sub[k] == v]\n",
    "\n",
    "    if sub.empty:\n",
    "        print(\"No data after filtering.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pivot = sub.pivot_table(\n",
    "        index=row_field,\n",
    "        columns=col_field,\n",
    "        values=value_field,\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "    if title:\n",
    "        print(f\"\\n{'=' * len(title)}\")\n",
    "        print(title)\n",
    "        print(f\"{'=' * len(title)}\")\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Table 1: Strategy x Scorer Grid ───────────────────────────────────────────\n",
    "\n",
    "for dataset_label in sorted(agg_df[\"project_label\"].dropna().unique()):\n",
    "    tbl = make_comparison_table(\n",
    "        agg_df,\n",
    "        row_field=\"scorer\",\n",
    "        col_field=\"strategy\",\n",
    "        value_field=\"exact_match_fmt\",\n",
    "        filter_dict={\"project_label\": dataset_label},\n",
    "        title=f\"Exact Match (%) — {dataset_label}\",\n",
    "    )\n",
    "    if not tbl.empty:\n",
    "        display(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Table 2: Aggregation x Scoring Window (beam search only) ──────────────────\n",
    "\n",
    "beam_df = agg_df[agg_df[\"strategy\"] == \"beam_search\"].copy()\n",
    "\n",
    "if beam_df.empty:\n",
    "    print(\"No beam search runs found.\")\n",
    "else:\n",
    "    for scorer in sorted(beam_df[\"scorer\"].dropna().unique()):\n",
    "        for dataset_label in sorted(beam_df[\"project_label\"].dropna().unique()):\n",
    "            tbl = make_comparison_table(\n",
    "                beam_df,\n",
    "                row_field=\"aggregation\",\n",
    "                col_field=\"scoring_window\",\n",
    "                value_field=\"exact_match_fmt\",\n",
    "                filter_dict={\"scorer\": scorer, \"project_label\": dataset_label},\n",
    "                title=f\"Beam Search — scorer={scorer}, dataset={dataset_label}\",\n",
    "            )\n",
    "            if not tbl.empty:\n",
    "                display(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Table 3: Compute Efficiency ───────────────────────────────────────────────\n",
    "\n",
    "eff_cols = [\"strategy\", \"scorer\", \"aggregation\", \"scoring_window\",\n",
    "            \"project_label\", \"model\",\n",
    "            \"exact_match_mean\", \"total_tflops_mean\",\n",
    "            \"avg_tokens_per_sample_mean\", \"avg_reasoning_steps_mean\"]\n",
    "present = [c for c in eff_cols if c in agg_df.columns]\n",
    "eff_df = agg_df[present].copy()\n",
    "\n",
    "# Rename for readability\n",
    "rename_map = {\n",
    "    \"exact_match_mean\": \"Accuracy (%)\",\n",
    "    \"total_tflops_mean\": \"Total TFLOPS\",\n",
    "    \"avg_tokens_per_sample_mean\": \"Tokens/Problem\",\n",
    "    \"avg_reasoning_steps_mean\": \"Reasoning Steps\",\n",
    "}\n",
    "eff_df.rename(columns={k: v for k, v in rename_map.items() if k in eff_df.columns},\n",
    "              inplace=True)\n",
    "\n",
    "eff_df.sort_values(\"Accuracy (%)\", ascending=False, inplace=True)\n",
    "print(\"Compute Efficiency Overview\")\n",
    "print(\"=\" * 40)\n",
    "display(eff_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LaTeX Export ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def to_latex(df: pd.DataFrame, caption: str, label: str) -> str:\n",
    "    \"\"\"Convert a DataFrame to a booktabs LaTeX table string.\"\"\"\n",
    "    latex = df.to_latex(\n",
    "        index=True,\n",
    "        escape=True,\n",
    "        na_rep=\"--\",\n",
    "        caption=caption,\n",
    "        label=label,\n",
    "        position=\"htbp\",\n",
    "    )\n",
    "    # Add booktabs rules\n",
    "    latex = latex.replace(\"\\\\toprule\", \"\\\\toprule\")  # already there with booktabs\n",
    "    return latex\n",
    "\n",
    "\n",
    "# Re-generate tables and export as LaTeX\n",
    "latex_outputs = []\n",
    "\n",
    "# Strategy x Scorer tables\n",
    "for dataset_label in sorted(agg_df[\"project_label\"].dropna().unique()):\n",
    "    tbl = make_comparison_table(\n",
    "        agg_df,\n",
    "        row_field=\"scorer\",\n",
    "        col_field=\"strategy\",\n",
    "        value_field=\"exact_match_fmt\",\n",
    "        filter_dict={\"project_label\": dataset_label},\n",
    "    )\n",
    "    if not tbl.empty:\n",
    "        ltx = to_latex(\n",
    "            tbl,\n",
    "            caption=f\"Exact match accuracy (\\\\%) by strategy and scorer on {dataset_label}.\",\n",
    "            label=f\"tab:strategy_scorer_{dataset_label}\",\n",
    "        )\n",
    "        latex_outputs.append((f\"Strategy x Scorer — {dataset_label}\", ltx))\n",
    "\n",
    "# Beam search aggregation x window tables\n",
    "if not beam_df.empty:\n",
    "    for scorer in sorted(beam_df[\"scorer\"].dropna().unique()):\n",
    "        for dataset_label in sorted(beam_df[\"project_label\"].dropna().unique()):\n",
    "            tbl = make_comparison_table(\n",
    "                beam_df,\n",
    "                row_field=\"aggregation\",\n",
    "                col_field=\"scoring_window\",\n",
    "                value_field=\"exact_match_fmt\",\n",
    "                filter_dict={\"scorer\": scorer, \"project_label\": dataset_label},\n",
    "            )\n",
    "            if not tbl.empty:\n",
    "                ltx = to_latex(\n",
    "                    tbl,\n",
    "                    caption=f\"Beam search accuracy (\\\\%) — scorer={scorer}, dataset={dataset_label}.\",\n",
    "                    label=f\"tab:beam_{scorer}_{dataset_label}\",\n",
    "                )\n",
    "                latex_outputs.append((f\"Beam {scorer} — {dataset_label}\", ltx))\n",
    "\n",
    "# Efficiency table\n",
    "if not eff_df.empty:\n",
    "    ltx = to_latex(\n",
    "        eff_df.reset_index(drop=True),\n",
    "        caption=\"Compute efficiency comparison across strategies.\",\n",
    "        label=\"tab:compute_efficiency\",\n",
    "    )\n",
    "    latex_outputs.append((\"Compute Efficiency\", ltx))\n",
    "\n",
    "# Print all LaTeX\n",
    "for title, ltx in latex_outputs:\n",
    "    print(f\"% ── {title} \" + \"─\" * (60 - len(title)))\n",
    "    print(ltx)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualization ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# Bar chart: accuracy by strategy (per dataset)\n",
    "plot_df = agg_df.dropna(subset=[\"exact_match_mean\"]).copy()\n",
    "\n",
    "if not plot_df.empty:\n",
    "    fig, axes = plt.subplots(\n",
    "        1, max(1, plot_df[\"project_label\"].nunique()),\n",
    "        figsize=(6 * max(1, plot_df[\"project_label\"].nunique()), 5),\n",
    "        squeeze=False,\n",
    "    )\n",
    "    for idx, dataset_label in enumerate(sorted(plot_df[\"project_label\"].unique())):\n",
    "        ax = axes[0, idx]\n",
    "        sub = plot_df[plot_df[\"project_label\"] == dataset_label]\n",
    "        # Average across scorers/configs per strategy\n",
    "        bars = sub.groupby(\"strategy\")[\"exact_match_mean\"].mean().sort_values()\n",
    "        bars.plot.barh(ax=ax, color=\"steelblue\")\n",
    "        ax.set_xlabel(\"Exact Match (%)\")\n",
    "        ax.set_title(dataset_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data for bar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap: beam search scorer x aggregation x window\n\nif sns is None:\n    print(\"Install seaborn for heatmap visualization: pip install seaborn\")\nelif not beam_df.empty:\n    heat_df = beam_df.dropna(subset=[\"exact_match_mean\"]).copy()\n    heat_df[\"config\"] = heat_df[\"aggregation\"].astype(str) + \" / w=\" + heat_df[\"scoring_window\"].astype(str)\n\n    for dataset_label in sorted(heat_df[\"project_label\"].dropna().unique()):\n        sub = heat_df[heat_df[\"project_label\"] == dataset_label]\n        if sub.empty:\n            continue\n        pivot = sub.pivot_table(\n            index=\"config\", columns=\"scorer\",\n            values=\"exact_match_mean\", aggfunc=\"first\",\n        )\n        if pivot.empty:\n            continue\n\n        fig, ax = plt.subplots(figsize=(max(6, pivot.shape[1] * 2), max(4, pivot.shape[0] * 0.6)))\n        sns.heatmap(pivot, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", ax=ax)\n        ax.set_title(f\"Beam Search Accuracy — {dataset_label}\")\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No beam search data for heatmap.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}