# @package _global_

# Self-Consistency Configuration for AIME 2025
# Competition-level mathematics problems from American Invitational Mathematics Examination

defaults:
  - /config  # Load main Hydra config
  - /dataset/aime_2025
  - /model/openrouter
  - /generation/default
  - /system/default
  - _self_

# Run naming for descriptive outputs
run_name: "aime2025_selfconsistency_gpt4omini_n${strategy.num_paths}_${now:%H-%M-%S}"

# Main configuration
verbose: false
report_to: wandb  # Enable wandb logging (optional)
wandb_project: llm-tts-eval-self-consistency  # Wandb project name

# Model configuration (API-based)
model:
  provider: openrouter
  model_name: "openai/gpt-4o-mini"
  api_key: null  # Set via OPENROUTER_API_KEY env var

# Generation settings
generation:
  max_new_tokens: 32000  # Large limit for complex AIME problems
  temperature: 0.7  # Diversity for self-consistency
  top_p: 1.0
  batch_size: 1

# No scorer needed (strategy has built-in majority voting)
scorer: null

# Self-consistency strategy
strategy:
  type: self_consistency
  num_paths: 16  # Number of reasoning paths (matching DeepConf budget)
  max_new_tokens: 32000
  temperature: 0.7  # Sampling temperature for diversity
  generation_batch_size: null  # Generate all at once
  n_threads: 8  # Parallel generation threads

# Evaluation configuration
evaluation:
  evaluators:
    - llm_judge
  llm_judge:
    provider: openrouter
    base_url: https://openrouter.ai/api/v1
    model: deepseek/deepseek-r1-0528
    cache_path: ~/.cache
    n_threads: 1
    prompt_file: null

# Dataset settings
dataset:
  subset: null  # Evaluate all AIME 2025 problems (30 total)
  prompt_file: "./config/prompts/aime_2025_self_consistency.txt"  # Few-shot CoT prompt (matching Wang et al. 2022)
