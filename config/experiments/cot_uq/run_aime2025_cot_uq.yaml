# @package _global_

# Tree-of-Thoughts Configuration for MATH Dataset
# Explores multiple reasoning paths via beam search with state evaluation

defaults:
  - /config  # Load main Hydra config
  - /dataset/aime_2025
  - /model/hf_qwen32
  - /strategy/cot_uq
  - /generation/default
  - /system/default
  - _self_

# Run naming for descriptive outputs
run_name: "aime25_cot_uq_budget${strategy.budget}_${now:%H-%M-%S}"

# Main configuration
verbose: false
report_to: wandb  # Enable wandb logging (optional)
wandb_project: llm-tts-eval-tot  # Wandb project name for ToT experiments

# Model configuration (API-based)
# model:
#   provider: openrouter
#   model_name: "openai/gpt-4o-mini"
#   api_key: null  # Set via OPENROUTER_API_KEY env var

model:
  type: "local"
  model_path: "Qwen/Qwen3-8B"
  disable_thinking_mode: true
  device: cuda:1

# Generation settings
generation:
  max_new_tokens: 5128  # Longer for MATH problems
  temperature: 0.7
  top_p: 1.0
  batch_size: 1
  max_steps: 10
  max_empty_steps: 5

# CoT-UQ strategy (uses model logprobs + COT importance)
# Backwards compatibility: keep top-level `scorer` key for scripts that
# expect `config.scorer.*` (some code reads `config.scorer.type`).
scorer: null

strategy:
  type: cot_uq
  # Sampling budget (number of full traces to sample)
  budget: 6

  # Generation parameters (will be forwarded to strategy)
  temperature: 0.7
  top_p: 0.95
  max_tokens: 2048
  top_logprobs: 10

  max_steps: 10
  max_empty_steps: 5

  # Combination weight between prob_conf and cot_importance (0..1)
  alpha: 0.5

  # Optional scorer configuration for CoT-UQ. When enabled, the strategy
  # manager will instantiate `CotUqScorer` and pass it into the strategy.
  scorer:
    enabled: true
    # Note: `alpha` and `top_logprobs` are read from the strategy top-level
    # settings (kept here for explicitness). Manager currently uses
    # `strategy.alpha` and `strategy.top_logprobs` when constructing the scorer.
    alpha: 0.5
    top_logprobs: 10

  detector_step_patterns: [
            "\n**Step",
            "\n## Step",
            "\n- Step",
            "- Step",
            "\nStep",
        ]
  detector_answer_patterns: [
            "<Answer>:",
            "<answer>:",
            "final answer",
            "Final answer",
            "Answer:",
            "Answer: ",
            "answer:",
            "Answer is",
            "answer is",
            "<end of response>",
            "<|im_end|>",
        ]

# Evaluation configuration
evaluation:
  evaluators:
    - exact_match  # Exact match evaluator for MATH
  #   - llm_judge
  # llm_judge:
  #   provider: openrouter
  #   base_url: https://openrouter.ai/api/v1
  #   model: deepseek/deepseek-r1-0528
  #   cache_path: ~/.cache
  #   n_threads: 1
  #   prompt_file: null

# Dataset settings
dataset:
  subset: 0  # Number of examples to evaluate
