# @package _global_

# Tree-of-Thoughts Configuration for Game of 24
# Reproduces original ToT paper (Yao et al., 2023) evaluation
# Paper results: ToT achieves 74% success rate vs 7.3% IO and 4% CoT on GPT-4

defaults:
  - /config  # Load main Hydra config
  - /dataset/game24
  - /model/openrouter
  - /strategy/tree_of_thoughts
  - /generation/default
  - /system/default
  - _self_

# Run naming for descriptive outputs
run_name: "game24_tot_gpt4_beam${strategy.beam_width}_${now:%H-%M-%S}"

# Main configuration
verbose: false
report_to: wandb  # Enable wandb logging (optional)
wandb_project: llm-tts-eval-tot  # Wandb project name for ToT experiments

# Model configuration (API-based)
# Using GPT-4 to match original ToT paper (Yao et al., 2023)
model:
  provider: openrouter
  model_name: "openai/gpt-4"
  api_key: null  # Set via OPENROUTER_API_KEY env var

# Generation settings (match paper)
generation:
  max_new_tokens: 500       # Game of 24 needs fewer tokens than math problems
  temperature: 0.7          # Paper: 0.7
  top_p: 1.0
  batch_size: 1

# ToT strategy (no scorer needed - uses internal state evaluation)
scorer: null

# Tree-of-Thoughts strategy configuration
# Parameters match original ToT paper (Yao et al., 2023)
# Paper settings for Game of 24:
# - method_generate: "propose" (sequential thought generation)
# - method_evaluate: "value" (state evaluation)
# - n_evaluate_sample: 3 (3 evaluation samples per state)
# - beam_width (n_select_sample): 5 (keep top 5 states)
strategy:
  type: tree_of_thoughts
  method_generate: propose  # Paper: "propose" for step-by-step generation
  method_evaluate: value    # Paper: "value" for state scoring
  steps: 4                  # Game of 24 requires exactly 3 steps + final answer
  # All other parameters inherited from base config (strategy/tree_of_thoughts.yaml)
  # which now matches the paper: n_evaluate_sample=3, temperature=0.7, beam_width=5

  # Game of 24 specific prompts (from original ToT paper)
  propose_prompt_path: ./config/prompts/game24_tot_propose_step.txt
  cot_prompt_path: ./config/prompts/game24_tot_propose.txt
  value_prompt_path: ./config/prompts/game24_tot_value.txt
  value_last_step_prompt_path: ./config/prompts/game24_tot_value_last_step.txt

# Evaluation configuration
# Game of 24 has deterministic correctness (expression evaluates to 24)
# We can use a simple evaluator that checks the math
evaluation:
  evaluators:
    - llm_judge
  llm_judge:
    provider: openrouter
    base_url: https://openrouter.ai/api/v1
    model: openai/gpt-4  # Use GPT-4 for verification
    cache_path: ~/.cache
    n_threads: 1
    prompt_file: null

# Dataset settings
# Paper uses indices 900-1000 (100 hard puzzles)
dataset:
  subset: 100  # Full paper evaluation (100 examples from indices 900-1000)
  start_index: 900  # Hard puzzles (85-90% human solve rate)
  end_index: 1000
