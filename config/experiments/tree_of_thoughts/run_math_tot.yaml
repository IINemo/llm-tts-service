# @package _global_
# Run: CUDA_VISIBLE_DEVICES=0 python scripts/run_tts_eval.py --config-path=../config --config-name=experiments/tree_of_thoughts/run_math_tot

# Tree-of-Thoughts Configuration for MATH Dataset
# Explores multiple reasoning paths via beam search with state evaluation

defaults:
  - /config  # Load main Hydra config
  - /dataset/math
  - /model/openrouter
  - /strategy/tree_of_thoughts
  - /generation/default
  - /system/default
  - _self_

# Run naming for descriptive outputs
run_name: "run_math_tot_seed${system.seed}_${now:%H-%M-%S}"

# Main configuration
verbose: false
report_to: wandb  # Enable wandb logging (optional)
wandb_project: llm-tts-eval-tot  # Wandb project name for ToT experiments
wandb_group: "tree_of_thoughts_math"

# Model configuration (API-based)
model:
  provider: openrouter
  model_name: "openai/gpt-4o-mini"
  api_key: null  # Set via OPENROUTER_API_KEY env var

# Generation settings
generation:
  max_new_tokens: 1024  # Longer for MATH problems
  temperature: 0.7
  top_p: 1.0
  batch_size: 1

# ToT strategy (no scorer needed - uses internal state evaluation)
scorer: null

# Tree-of-Thoughts strategy configuration
strategy:
  type: tree_of_thoughts
  method_generate: propose  # "propose" or "sample"
  method_evaluate: value    # "value" or "vote"
  beam_width: 5            # Number of states to keep at each step
  steps: 6                 # More steps for harder problems
  n_generate_sample: 5     # Candidates per state
  n_evaluate_sample: 3     # Evaluation samples per candidate
  temperature: 0.7
  max_tokens_per_step: 150  # Longer steps for complex math
  n_threads: 8
  cache_values: true

# Evaluation configuration
evaluation:
  evaluators:
    - llm_judge
  llm_judge:
    provider: openrouter
    base_url: https://openrouter.ai/api/v1
    model: deepseek/deepseek-r1-0528
    cache_path: ~/.cache
    n_threads: 1
    prompt_file: null

# Dataset settings
dataset:
  subset: 10  # Number of examples to evaluate
