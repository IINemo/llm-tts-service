# @package generation

temperature: 0.7  # Generation temperature
max_new_tokens: 500  # Max tokens per step
max_length: 8196  # Max length of the tokenized input
batch_size: 1  # Batch size for candidate generation (null = same as n)
sequential_generation: false  # Generate candidates one by one to save memory
top_p: 0.8
top_k: 20
presence_penalty: 0.0  # Penalty for repeated tokens (0.0 = no penalty)
checkpoint_batch_size: 32  # Save intermediate results every N samples (for long-running jobs)
