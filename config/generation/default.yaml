# @package generation

temperature: 0.7  # Generation temperature
max_new_tokens: 500  # Max tokens per step
max_length: 8096  # Max length of the tokenized input
batch_size: 2  # Batch size for candidate generation (null = same as n)
sequential_generation: false  # Generate candidates one by one to save memory
top_p: 0.95
top_k: 50
