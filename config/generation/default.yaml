# @package generation

temperature: 0.7  # Generation temperature
max_new_tokens: 500  # Max tokens per step
max_length: 8196  # Max length of the tokenized input
batch_size: 1  # Batch size for candidate generation (null = same as n)
sequential_generation: false  # Generate candidates one by one to save memory
top_p: 0.8
top_k: 20
