# @package model

type: "vllm"
model_path: "Qwen/Qwen3-8B"
model_short_name: "qwen3_8b"  # Short name for output directory organization
device: cuda

# vLLM-specific settings
gpu_memory_utilization: 0.9
tensor_parallel_size: 1
enable_prefix_caching: true
trust_remote_code: true
max_context_budget: 32768
