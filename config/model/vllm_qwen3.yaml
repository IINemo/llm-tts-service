# @package model

type: "vllm"
model_path: "Qwen/Qwen3-8B"
device: cuda

# vLLM-specific settings
gpu_memory_utilization: 0.9
tensor_parallel_size: 1
enable_prefix_caching: true
trust_remote_code: true
max_model_len: 32768
