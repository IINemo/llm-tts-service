{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e242606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quang.nguyen/miniconda3/envs/mur/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 23:31:52 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 10-03 23:31:52 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-1.7B', 'gpu_memory_utilization': 0.5, 'disable_log_stats': True}\n",
      "INFO 10-03 23:31:58 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 10-03 23:31:58 [__init__.py:1750] Using max model len 40960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 23:31:58,146\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 23:31:58 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:31:59 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:31:59 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-1.7B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:00 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m WARNING 10-03 23:32:00 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:00 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-1.7B...\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:01 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:01 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:01 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.55it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:02 [default_loader.py:262] Loading weights took 0.67 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:03 [gpu_model_runner.py:2007] Model loading took 3.2152 GiB and 1.926064 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:06 [backends.py:548] Using cache directory: /home/quang.nguyen/.cache/vllm/torch_compile_cache/b1ac2804e2/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:06 [backends.py:559] Dynamo bytecode transform time: 3.42 s\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:09 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.188 s\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:10 [monitor.py:34] torch.compile takes 3.42 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:10 [gpu_worker.py:276] Available KV cache memory: 19.10 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:11 [kv_cache_utils.py:849] GPU KV cache size: 178,816 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:11 [kv_cache_utils.py:853] Maximum concurrency for 40,960 tokens per request: 4.37x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 59.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:12 [gpu_model_runner.py:2708] Graph capturing finished in 1 secs, took 0.49 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=709356)\u001b[0;0m INFO 10-03 23:32:12 [core.py:214] init engine (profile, create kv cache, warmup model) took 9.23 seconds\n",
      "INFO 10-03 23:32:13 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from vllm import LLM\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen3-1.7B\"\n",
    "vllm = LLM(model=model_name_or_path, gpu_memory_utilization=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f279732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from llm_tts.strategies import MUR_No_Critic\n",
    "\n",
    "from lm_polygraph.estimators import Perplexity\n",
    "estimator = Perplexity()\n",
    "mur = MUR_No_Critic(vllm_model=vllm, estimators=estimator, max_steps=10, max_new_tokens=128, temperature=0.6,generation_batch_size=1, candidate_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700937c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1821.23it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s, est. speed input: 255.20 toks/s, output: 155.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1639.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s, est. speed input: 1120.54 toks/s, output: 181.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1948.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s, est. speed input: 860.49 toks/s, output: 182.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1880.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s, est. speed input: 626.59 toks/s, output: 181.91 toks/s]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 1862.69it/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  5.45it/s, est. speed input: 1352.94 toks/s, output: 492.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1597.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s, est. speed input: 1888.67 toks/s, output: 179.59 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$'''\n",
    "result = mur.generate_trajectory(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f74288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step0: To convert a point from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\theta)$, we use the following formulas:\n",
      "- $ r = \\sqrt{x^2 + y^2} $\n",
      "- $ \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right) $\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step1: Given the rectangular coordinates $(0, 3)$, we identify $x = 0$ and $y = 3$.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step2: Compute $ r $:\n",
      "$$\n",
      "r = \\sqrt{(0)^2 + (3)^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3\n",
      "$$\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step3: Compute $ \\theta $:\n",
      "$$\n",
      "\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right)\n",
      "$$\n",
      "Since $ \\frac{3}{0} $ is undefined, and $ x = 0 $, this means the point lies on the positive y-axis, and the angle is $ \\frac{\\pi}{2} $ radians.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step4: The polar coordinates are $(3, \\frac{\\pi}{2})$.\n",
      "the answer is $(3, \\frac{\\pi}{2})$\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for o in result['trajectory']:\n",
    "    print(o)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af0a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb02c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
