{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c319f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"<YOUR KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d69cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374aa247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c889e40a17d41eba6f6c0441b86a973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-7B-PRM800K were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from llm_tts.step_candidate_generator_through_api import StepCandidateGeneratorThroughAPI\n",
    "from llm_tts.models.blackboxmodel_with_streaming import BlackboxModelWithStreaming\n",
    "from llm_tts.scorers.step_scorer_prm import StepScorerPRM\n",
    "from llm_tts.strategies import StrategyOnlineBestOfN\n",
    "from llm_tts.step_boundary_detector import StepBoundaryDetector\n",
    "\n",
    "\n",
    "max_new_tokens = 100\n",
    "step_patterns = [\"- Step\", \"<Answer>:\", \"\\n<Answer>:\"]\n",
    "answer_patterns = [\"<Answer>:\", \"\\n<Answer>:\"]\n",
    "candidates_per_step = 3\n",
    "max_steps = 3\n",
    "generation_batch_size = 1\n",
    "\n",
    "llm = BlackboxModelWithStreaming(model_path = \"gpt-5-mini\", openai_api_key = OPENAI_API_KEY)\n",
    "step_boundary_detector = StepBoundaryDetector(\n",
    "    step_patterns=step_patterns, \n",
    "    answer_patterns=answer_patterns, \n",
    "    max_tokens_per_step=max_new_tokens)\n",
    "step_generator = StepCandidateGeneratorThroughAPI(llm, step_boundary_detector, prefill_mode=False)\n",
    "prm_model_path = \"Qwen/Qwen2.5-Math-7B-PRM800K\"\n",
    "scorer = StepScorerPRM(prm_model_path=prm_model_path, device=\"cuda:0\", batch_size=1)\n",
    "strategy = StrategyOnlineBestOfN(\n",
    "    step_generator=step_generator, \n",
    "    scorer=scorer, \n",
    "    candidates_per_step=candidates_per_step, \n",
    "    max_steps=max_steps,\n",
    "    generation_batch_size=generation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2daf702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You will be presented with a <Question>. Before providing the [Answer], you should first think step-by-step carefully.\n",
    "\n",
    "Your response format:\n",
    "<start of response>\n",
    "Reasoning Steps:\n",
    "- Step 1: Your reasoning step 1\n",
    "- Step 2: Your reasoning step 2\n",
    "- Step 3: Your reasoning step 3\n",
    "...\n",
    "- Step N: Your reasoning step N\n",
    "<Answer>: Your final answer\n",
    "<end of response>\n",
    "\n",
    "Follow the above output format STRICTLY! Do not add any other additional texts outside the template.\n",
    "Keep each reasoning step concise (single steps should not be too long).\n",
    "Each reasoning step must be on a single line (no line breaks within a step).\n",
    "\n",
    "Now answer:\n",
    "<Question>: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_request(question):\n",
    "    request = [\n",
    "                {\"role\": \"system\", \"content\": \"\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_template.format(question=question)\n",
    "                },\n",
    "            ]\n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ac1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trajectory': \"<start of response>\\nReasoning Steps:\\n- Step 1: Tom initially has 8 apples.\\n- Step 2: He gave 3 to his friend, leaving 8 - 3 = 5 apples.\\n- Step 3: He bought 5 more, so 5 + 5 = 10 apples.\\n\\n<Answer>:\\n<start of response>\\nReasoning Steps:\\n- Step 1: I can't share internal chain-of-thought, but I can give the result and a brief summary.\",\n",
       " 'steps': [StepCandidate(), StepCandidate(), StepCandidate(), StepCandidate()],\n",
       " 'validity_scores': [0.85546875],\n",
       " 'completed': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Tom had 8 apples. He gave 3 to his friend and bought 5 more. How many apples does Tom have now?\"\n",
    "\n",
    "request = create_request(question)\n",
    "result = strategy.generate_trajectory(request)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
