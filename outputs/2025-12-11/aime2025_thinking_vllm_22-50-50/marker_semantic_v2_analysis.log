=============================================================================
MARKER_SEMANTIC_V2 ANALYSIS LOG
=============================================================================
Date: 2025-12-12
Goal: Improve marker_semantic detector to better match GPT-4.1 step boundaries

=============================================================================
STEP 1: ANALYZE GPT-4.1 STEP STARTING WORDS
=============================================================================

Analyzed 1084 GPT-4.1 steps to find what words/phrases it uses to start steps.

TOP SINGLE WORDS (step starters):
   128x  'wait'
   114x  'so'
   105x  'let' (usually "let me")
   102x  'but'
    89x  'alternatively'
    68x  'for' (usually "for example", "for m", "for n")
    50x  'therefore'
    25x  'the'
    22x  'if'
    21x  'now'
    17x  'first'
    17x  'however'
    11x  'this'
    10x  'similarly'
    10x  'because'

TOP BIGRAMS:
   100x  'let me'
    48x  'wait but'
    43x  'so the'
    34x  'alternatively maybe'
    26x  'therefore the'
    18x  'alternatively let'
    18x  'for example'
    16x  'alternatively perhaps'
    15x  'wait let'
    11x  'wait no'
     9x  'wait maybe'
     9x  'but wait'
     7x  'given that'

TOP TRIGRAMS:
    19x  'let me think'
    16x  'alternatively let me'
    16x  'let me compute'
    13x  'wait let me'
    12x  'wait but the'

=============================================================================
STEP 2: COMPARE WITH CURRENT MARKER_SEMANTIC
=============================================================================

Current marker_semantic already covers:
  ✓ 'wait' (128x)
  ✓ 'so' (114x)
  ✓ 'let me' (105x)
  ✓ 'therefore' (50x)
  ✓ 'first' (17x)
  ✓ 'then', 'next', 'finally', etc.

MISSING from marker_semantic (GPT-4.1 uses but we don't):
  ✗ 'but' (102x)
  ✗ 'alternatively' (89x)
  ✗ 'for example' (18x)
  ✗ 'however' (17x)
  ✗ 'now' (21x)
  ✗ 'similarly' (10x)
  ✗ 'because' (10x)
  ✗ 'given that' (7x)

=============================================================================
STEP 3: FIRST ATTEMPT - ADD ALL MISSING MARKERS
=============================================================================

Attempted to add:
- SAFE ANYWHERE: alternatively, for example, similarly, given that, suppose,
                 wait but, wait no, wait maybe
- SENTENCE-START ONLY: but, however, because (only after .!?\n)

RESULT: OVER-SPLITTING
  marker_semantic:    207.6 steps/trace (4.2% diff from GPT-4.1)
  marker_semantic_v2: 241.8 steps/trace (11.5% diff from GPT-4.1) <- WORSE!

CONCLUSION: Adding too many markers causes over-splitting.
GPT-4.1 is more selective - it doesn't split on EVERY occurrence of "but" or
"alternatively", only when they represent true reasoning transitions.

=============================================================================
STEP 4: ANALYZE WHERE GPT-4.1 SPLITS THAT WE MISS
=============================================================================

For traces where GPT-4.1 has MORE steps than marker_semantic, analyzed what
step starts marker_semantic misses:

Step starts GPT-4.1 uses that marker_semantic MISSES:
   14x  'alternatively, maybe'
   14x  'for example,'
    9x  'but the'
    9x  'but this'
    9x  'alternatively, think'
    8x  'but maybe'
    8x  'because if'
    6x  'but how'
    6x  'alternatively, perhaps'
    6x  'but if'
    5x  'but since'
    3x  'however, the'
    3x  'suppose we'

=============================================================================
STEP 5: SELECTIVE APPROACH - ONLY MULTI-WORD PHRASES
=============================================================================

Key insight: Single words like "but", "however", "alternatively" cause
over-splitting because they appear mid-sentence too often.

Solution: Only add MULTI-WORD PHRASES that are unambiguous step starters:
- "for example" - always starts an example
- "given that" - always starts a premise
- "similarly" - always starts a comparison

These phrases are:
1. Specific enough to indicate true reasoning transitions
2. Rarely appear mid-sentence as noise
3. Used frequently by GPT-4.1 as step boundaries

=============================================================================
STEP 6: FINAL RESULTS
=============================================================================

marker_semantic_v2 = marker_semantic + ["for example", "given that", "similarly"]

COMPARISON:
===========================================================================
Metric                  marker_semantic  marker_semantic_v2  llm_gpt4
---------------------------------------------------------------------------
total_steps                        1038               1075      1084
avg_steps_per_trace               207.6              215.0     216.8
avg_step_length                   186.8              180.1     179.1
median_step_length                  155                152       116
===========================================================================

IMPROVEMENT:
  marker_semantic:    4.2% diff from GPT-4.1
  marker_semantic_v2: 0.8% diff from GPT-4.1  <- 5x BETTER!

PER-TRACE COMPARISON:
  Sample 1: GPT4=237, v1=269, v2=274 | v1_diff=+32, v2_diff=+37
  Sample 2: GPT4= 69, v1= 58, v2= 59 | v1_diff=-11, v2_diff=-10
  Sample 3: GPT4=249, v1=242, v2=245 | v1_diff= -7, v2_diff= -4
  Sample 4: GPT4=264, v1=233, v2=256 | v1_diff=-31, v2_diff= -8  <- Big improvement!
  Sample 5: GPT4=265, v1=236, v2=241 | v1_diff=-29, v2_diff=-24

=============================================================================
KEY LESSONS LEARNED
=============================================================================

1. MORE MARKERS != BETTER
   Adding all missing markers (but, however, alternatively, etc.) caused
   over-splitting because these words appear frequently mid-sentence.

2. GPT-4.1 IS SELECTIVE
   GPT-4.1 doesn't split on every "but" - it uses semantic understanding
   to determine when a word represents a true reasoning transition.

3. MULTI-WORD PHRASES ARE SAFER
   Phrases like "for example", "given that" are unambiguous because they
   rarely appear mid-sentence as noise.

4. SENTENCE-START CONSTRAINTS AREN'T ENOUGH
   Even with (?<=[.!?\n])\s*\bbut\b pattern, "but" still over-splits
   because many sentences legitimately start with "but" without being
   new reasoning steps.

5. THE 80/20 RULE APPLIES
   Adding just 3 selective markers ("for example", "given that", "similarly")
   improved alignment from 4.2% to 0.8% - most of the gain came from
   the most specific phrases.

=============================================================================
MARKERS NOT WORTH ADDING (cause over-splitting)
=============================================================================

- "but" - too common, even at sentence start
- "however" - common sentence starter
- "alternatively" - appears too frequently in math reasoning
- "because" - common mid-sentence
- "now" - ambiguous (time reference vs transition)
- Single-word markers in general

=============================================================================
FINAL MARKER_SEMANTIC_V2 DEFINITION
=============================================================================

All original marker_semantic markers PLUS:
- r"\bfor example\b"
- r"\bgiven that\b"
- r"\bsimilarly\b"

Parameters: min_step_chars=100, max_step_chars=600

=============================================================================

=============================================================================
STEP 7: STEP CONTENT ALIGNMENT ANALYSIS
=============================================================================

Compared actual step content between GPT-4.1 and marker_semantic_v2.

CONTENT SIMILARITY:
  - Matched steps (>50% similar): 682/1084 (62.9%)
  - High matches (>70% similar): 334/1084 (30.8%)

BOUNDARY POSITION ALIGNMENT:
  Per-sample analysis (boundaries within 100 chars):
    Sample 1: 78% boundaries align
    Sample 2: 99% boundaries align  
    Sample 3: 91% boundaries align
    Sample 4: 80% boundaries align
    Sample 5: 91% boundaries align

  Overall:
    - Exact matches (within 10 chars): 144/1079 (13.3%)
    - Close matches (within 100 chars): 927/1079 (85.9%)

KEY INSIGHT: 85.9% of GPT-4.1 boundaries have a marker_semantic_v2 boundary
within 100 characters. The detectors split at similar locations, just with
slight offsets due to different marker positions.

=============================================================================
STEP 8: ANALYSIS OF MISSED BOUNDARIES
=============================================================================

Looking at boundaries GPT-4.1 creates that marker_semantic_v2 misses:

Most missed splits happen at "Alternatively" - GPT-4.1 frequently splits on
this word, but marker_semantic_v2 doesn't include it (intentionally, to avoid
over-splitting).

Examples of GPT-4.1 splits that v2 misses:
  - "...behavior. Alternatively, since the function..."
  - "...complicated here. Alternatively, maybe we can..."
  - "...at a minimum. Alternatively, maybe the function..."
  - "...equation, perhaps? Alternatively, maybe we can..."

This confirms our earlier finding: "alternatively" is a high-frequency
GPT-4.1 splitter (89x), but adding it causes over-splitting in rule-based
detection because GPT-4.1 is selective about WHEN to split on it.

=============================================================================
CONCLUSION: ALIGNMENT QUALITY
=============================================================================

STATISTICAL ALIGNMENT: 0.8% difference in avg steps/trace
BOUNDARY ALIGNMENT: 85.9% of boundaries within 100 chars
CONTENT ALIGNMENT: 62.9% of steps match (>50% similar)

The 0.8% statistical match doesn't mean identical steps - it means the
DISTRIBUTION of step sizes is similar. The actual step boundaries differ
because:

1. GPT-4.1 uses semantic understanding to decide WHEN to split
2. marker_semantic_v2 uses pattern matching (always splits on markers)
3. GPT-4.1 splits on "alternatively" selectively, we don't at all

TRADE-OFF:
- Adding "alternatively" would improve boundary alignment
- But would worsen statistical alignment (over-splitting)
- Current balance (0.8% stat diff, 86% boundary alignment) is good

=============================================================================
